import os
from typing import List

from groq import AsyncGroq, Groq
from uac.configs.config import Config


class GroqClient:
    """
    A client for interfacing with Groq's API to generate responses from language models.

    This class provides synchronous and asynchronous methods for generating responses using
    Groq's language models.

    Attributes:
    -----------
    aclient : AsyncGroq
        An instance of the asynchronous Groq client.
    client : Groq
        An instance of the synchronous Groq client.
    model : str
        The name of the model to be used, as specified in the configuration.

    Methods:
    --------
    __init__(config: Config):
        Initializes the GroqClient with the provided configuration.
    aclient_response(model_id: str, messages: list, max_tokens: int = 1024) -> str:
        Asynchronously generates a response from the specified model based on a list of input messages.
    client_response(model_id: str, messages: list, max_tokens: int = 1024) -> str:
        Generates a response from the specified model based on a list of input messages.
    """

    def __init__(self, config: Config):
        """
        Initializes the GroqClient with the provided configuration.

        Parameters:
        -----------
        config : Config
            The configuration object containing settings for the client, including the model name.
        """
        self.aclient = AsyncGroq(api_key=os.environ["GROQ_API_KEY"])
        self.client = Groq(api_key=os.environ["GROQ_API_KEY"])

    async def toolcall_response(
        self,
        model_id: str,
        messages: list,
        max_tokens: int = 1024,
        tools: List = None,
        tool_choice: str = None,
    ) -> str:
        """
        Asynchronously generates a response from the specified model based on a list of input messages.

        Parameters:
        -----------
        model_id : str
            The identifier of the model to be used.
        messages : list
            A list of input messages to be processed by the model.
        max_tokens : int, optional
            The maximum number of tokens to generate in the response (default is 1024).
        tools: list
            A list of tools for calling agent
        tool_choice: str
            The choice mode of tool to use for calling agent eg.[none, auto, required]

        Returns:
        message: str
            Message instant of OpenAI Completions API
        """
        chat_completion = await self.aclient.chat.completions.create(
            messages=messages,
            model=model_id,
            max_tokens=max_tokens,
            tools=tools,
            tool_choice=tool_choice,
        )

        message = chat_completion.choices[0].message

        return message

    async def aclient_response(
        self, model_id: str, messages: list, max_tokens: int = 1024, **kwargs
    ) -> str:
        """
        Asynchronously generates a response from the specified model based on a list of input messages.

        Parameters:
        -----------
        model_id : str
            The identifier of the model to be used.
        messages : list
            A list of input messages to be processed by the model.
        max_tokens : int, optional
            The maximum number of tokens to generate in the response (default is 1024).

        Returns:
        --------
        str
            The response generated by the model based on the input messages.

        Example:
        --------
        >>> response = await aclient_response("model-id", [{"role": "user", "content": "Hello"}])
        >>> print(response)
        "Hello! How can I assist you today?"
        """
        if kwargs.get("streaming"):
            return await self.aclient.chat.completions.create(
                messages=messages,
                model=model_id,
                temperature=0.1,
                top_p=0.9,
                max_tokens=max_tokens,
                stream=True,
            )
        chat_completion = await self.aclient.chat.completions.create(
            messages=messages,
            model=model_id,
            temperature=0.1,
            top_p=0.9,
            max_tokens=max_tokens,
        )

        content = chat_completion.choices[0].message.content

        return content

    def client_response(
        self, model_id: str, messages: list, max_tokens: int = 1024, **kwargs
    ) -> str:
        """
        Generates a response from the specified model based on a list of input messages.

        Parameters:
        -----------
        model_id : str
            The identifier of the model to be used.
        messages : list
            A list of input messages to be processed by the model.
        max_tokens : int, optional
            The maximum number of tokens to generate in the response (default is 1024).

        Returns:
        --------
        str
            The response generated by the model based on the input messages.

        Example:
        --------
        >>> response = client_response("model-id", [{"role": "user", "content": "Hello"}])
        >>> print(response)
        "Hello! How can I assist you today?"
        """
        if kwargs.get("streaming"):
            return self.client.chat.completions.create(
                messages=messages,
                model=model_id,
                temperature=0.1,
                top_p=0.9,
                max_tokens=max_tokens,
                stream=True,
            )
        chat_completion = self.client.chat.completions.create(
            messages=messages,
            model=model_id,
            temperature=0.1,
            top_p=0.9,
            max_tokens=max_tokens,
        )

        content = chat_completion.choices[0].message.content

        return content
