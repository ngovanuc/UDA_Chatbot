import asyncio
from typing import List, Literal

from uac.configs.config import Config
from uac.llms.llms import LLM


class Base:
    """
    A base class for creating a tutoring system that interacts with language models.

    This class provides methods for generating responses from a specified language model
    and building structured message lists for conversation management.

    Attributes:
    -----------
    model : str
        The name of the model to be used, as specified in the configuration.
    client : LLM
        An instance of the LLM class initialized with the specified model ID.

    Methods:
    --------
    __init__(config: Config):
        Initializes the BaseTutor with the provided configuration.
    async client_response(messages: list, max_tokens: int = 1024) -> str:
        Asynchronously generates a response from the specified model based on a list of input messages.
    _build_messages(user_input: str, conversation: list = None, system_prompt: str = None) -> list:
        Builds a list of messages for the conversation, including the system prompt, previous conversation messages, and the latest user input.
    """

    def __init__(
        self,
        config: Config,
        mode: Literal["generate", "function_call", "preprocess", "extraction"] = "generate",
    ):
        """
        Initializes the BaseTutor with the provided configuration.

        Parameters:
        -----------
        config : Config
            The configuration object containing settings for the tutor, including the model name.
        """
        if mode == "generate":
            self.client = LLM(model_id=config.generate_model_name)
        elif mode == "function_call":
            self.client = LLM(model_id=config.function_calling_model_name)
        elif mode == "preprocess":
            self.client = LLM(model_id=config.preprocess_model_name)
        elif mode == "extraction":
            self.client = LLM(model_id=config.extraction_model_name)
        self.config = config

    async def aclient_response(
        self, messages: list, max_tokens: int = 1024, temperature: float = 0.1, **kwargs
    ) -> str:
        """
        Asynchronously generates a response from the specified model based on a list of input messages.

        Parameters:
        -----------
        messages : list
            A list of input messages to be processed by the model.
        max_tokens : int, optional
            The maximum number of tokens to generate in the response (default is 1024).

        Returns:
        --------
        str
            The response generated by the model based on the input messages.

        Example:
        --------
        >>> response = await client_response([{"role": "user", "content": "Hello"}])
        >>> print(response)
        "Hello! How can I assist you today?"
        """
        content = asyncio.run(
            self.client.aclient_response(
                messages=messages, max_tokens=max_tokens, temperature=temperature, **kwargs
            )
        )
        return content

    async def toolcall_response(
        self,
        messages: list,
        max_tokens: int = 1024,
        tools: List = None,
        tool_choice: str = None,
    ) -> str:
        """
        Asynchronously generates a response from the specified model based on a list of input messages.

        Parameters:
        -----------
        messages : list
            A list of input messages to be processed by the model.
        max_tokens : int, optional
            The maximum number of tokens to generate in the response (default is 1024).

        Returns:
        --------
        str
            The response generated by the model based on the input messages.

        Example:
        --------
        >>> response = await client_response([{"role": "user", "content": "Hello"}])
        >>> print(response)
        "Hello! How can I assist you today?"
        """
        message = asyncio.run(
            self.client.toolcall_response(messages, max_tokens, tools, tool_choice)
        )
        return message

    def client_response(self, messages: list, max_tokens: int = 1024, **kwargs) -> str:
        """
        Generates a response from the specified model based on a list of input messages.

        Parameters:
        -----------
        messages : list
            A list of input messages to be processed by the model.
        max_tokens : int, optional
            The maximum number of tokens to generate in the response (default is 1024).

        Returns:
        --------
        str
            The response generated by the model based on the input messages.

        Example:
        --------
        >>> response = await client_response([{"role": "user", "content": "Hello"}])
        >>> print(response)
        "Hello! How can I assist you today?"
        """
        content = self.client.client_response(messages, max_tokens, **kwargs)
        return content

    def _build_messages(
        self,
        user_input: str,
        conversation: list = None,
        system_prompt: str = None,
        retrieval_context: str = None,
    ) -> list:
        messages = []

        system_message = {"role": "system", "content": system_prompt}
        latest_user_message = {"role": "user", "content": user_input}
        if retrieval_context is not None:
            latest_user_message = {
                "role": "user",
                "content": retrieval_context
                + "\nDựa vào nội dung trích xuất này để trả lời câu hỏi sau:\n"
                + user_input,
            }

            if conversation is not None:
                messages.append(system_message)
                messages.extend(conversation)
                messages.append(latest_user_message)
            else:
                messages.extend([system_message, latest_user_message])
        else:
            if conversation is not None:
                messages.append(system_message)
                messages.extend(conversation)
                messages.append(latest_user_message)
            else:
                messages.extend([system_message, latest_user_message])
        return messages

    def run_agent(self):
        """Placeholder method to run the agent, to be implemented by subclasses."""
        pass
